{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas-datareader in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.10.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas-datareader) (5.3.0)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas-datareader) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas-datareader) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.23->pandas-datareader) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.23->pandas-datareader) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.23->pandas-datareader) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.23->pandas-datareader) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->pandas-datareader) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->pandas-datareader) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->pandas-datareader) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->pandas-datareader) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas>=0.23->pandas-datareader) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting BeautifulSoup\n",
      "  Downloading BeautifulSoup-3.2.2.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [7 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\admin\\AppData\\Local\\Temp\\pip-install-2kr656ep\\beautifulsoup_ba713abf62e54f198ff4a59799a95087\\setup.py\", line 3\n",
      "          \"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"<>\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n",
      "                                                                                                         ^^\n",
      "      SyntaxError: invalid syntax\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rownames</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>status</th>\n",
       "      <th>category</th>\n",
       "      <th>wind</th>\n",
       "      <th>pressure</th>\n",
       "      <th>tropicalstorm_force_diameter</th>\n",
       "      <th>hurricane_force_diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Amy</td>\n",
       "      <td>1975</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>-79.0</td>\n",
       "      <td>tropical depression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>1013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Amy</td>\n",
       "      <td>1975</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>28.5</td>\n",
       "      <td>-79.0</td>\n",
       "      <td>tropical depression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>1013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Amy</td>\n",
       "      <td>1975</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>29.5</td>\n",
       "      <td>-79.0</td>\n",
       "      <td>tropical depression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>1013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Amy</td>\n",
       "      <td>1975</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>30.5</td>\n",
       "      <td>-79.0</td>\n",
       "      <td>tropical depression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>1013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Amy</td>\n",
       "      <td>1975</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>31.5</td>\n",
       "      <td>-78.8</td>\n",
       "      <td>tropical depression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>1012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Amy</td>\n",
       "      <td>1975</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>32.4</td>\n",
       "      <td>-78.7</td>\n",
       "      <td>tropical depression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>1012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Amy</td>\n",
       "      <td>1975</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>33.3</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>tropical depression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>1011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Amy</td>\n",
       "      <td>1975</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-77.0</td>\n",
       "      <td>tropical depression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>1006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Amy</td>\n",
       "      <td>1975</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>34.4</td>\n",
       "      <td>-75.8</td>\n",
       "      <td>tropical storm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35</td>\n",
       "      <td>1004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Amy</td>\n",
       "      <td>1975</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-74.8</td>\n",
       "      <td>tropical storm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>1002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rownames name  year  month  day  hour   lat  long               status  \\\n",
       "0         1  Amy  1975      6   27     0  27.5 -79.0  tropical depression   \n",
       "1         2  Amy  1975      6   27     6  28.5 -79.0  tropical depression   \n",
       "2         3  Amy  1975      6   27    12  29.5 -79.0  tropical depression   \n",
       "3         4  Amy  1975      6   27    18  30.5 -79.0  tropical depression   \n",
       "4         5  Amy  1975      6   28     0  31.5 -78.8  tropical depression   \n",
       "5         6  Amy  1975      6   28     6  32.4 -78.7  tropical depression   \n",
       "6         7  Amy  1975      6   28    12  33.3 -78.0  tropical depression   \n",
       "7         8  Amy  1975      6   28    18  34.0 -77.0  tropical depression   \n",
       "8         9  Amy  1975      6   29     0  34.4 -75.8       tropical storm   \n",
       "9        10  Amy  1975      6   29     6  34.0 -74.8       tropical storm   \n",
       "\n",
       "   category  wind  pressure  tropicalstorm_force_diameter  \\\n",
       "0       NaN    25      1013                           NaN   \n",
       "1       NaN    25      1013                           NaN   \n",
       "2       NaN    25      1013                           NaN   \n",
       "3       NaN    25      1013                           NaN   \n",
       "4       NaN    25      1012                           NaN   \n",
       "5       NaN    25      1012                           NaN   \n",
       "6       NaN    25      1011                           NaN   \n",
       "7       NaN    30      1006                           NaN   \n",
       "8       NaN    35      1004                           NaN   \n",
       "9       NaN    40      1002                           NaN   \n",
       "\n",
       "   hurricane_force_diameter  \n",
       "0                       NaN  \n",
       "1                       NaN  \n",
       "2                       NaN  \n",
       "3                       NaN  \n",
       "4                       NaN  \n",
       "5                       NaN  \n",
       "6                       NaN  \n",
       "7                       NaN  \n",
       "8                       NaN  \n",
       "9                       NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\n",
    "    \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", nrows=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"><head>\\n\\n<meta charset=\"utf-8\">\\n<meta name=\"generator\" content=\"quarto-1.5.56\">\\n\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\">\\n\\n<meta name=\"author\" content=\"Arthur Turrell\">\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://aeturrell.com/research\"\n",
    "page = requests.get(url)\n",
    "page.text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       </div>\n",
      "          <div class=\"project-category\">\n",
      "           <a href=\"#category=gender pay gap\">\n",
      "            gender pay gap\n",
      "           </a>\n",
      "          </div>\n",
      "          <div class=\"project-category\">\n",
      "           <a href=\"#category=labour\">\n",
      "            labour\n",
      "           </a>\n",
      "          </div>\n",
      "          <div class=\"project-category\">\n",
      "           <a href=\"#category=text analysis\">\n",
      "            text analysis\n",
      "           </a>\n",
      "          </div>\n",
      "         </div>\n",
      "         <div class=\"project-details-listing\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "print(soup.prettify()[60000:60500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" <i>Environment and Planning B: Urban Analytics and City Science</i> (2024): 23998083241267331. doi: <a href=\"https://doi.org/10.1177/23998083241267331\"><code>10.1177/23998083241267331</code></a></p>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all paragraphs\n",
    "all_paras = soup.find_all(\"p\")\n",
    "# Just show one of the paras\n",
    "all_paras[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_paras[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331',\n",
       " 'Kalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and Sujit Kapadia. \"Making text count: economic forecasting using newspaper text.\" Journal of Applied Econometrics 37, no. 5 (2022): 896-919. doi: 10.1002/jae.2907',\n",
       " 'Turrell, A., Speigner, B., Copple, D., Djumalieva, J. and Thurgood, J., 2021. Is the UK’s productivity puzzle mostly driven by occupational mismatch? An analysis using big data on job vacancies. Labour Economics, 71, p.102013. doi: 10.1016/j.labeco.2021.102013',\n",
       " 'Haldane, Andrew G., and Arthur E. Turrell. \"Drawing on different disciplines: macroeconomic agent-based models.\" Journal of Evolutionary Economics 29 (2019): 39-66. doi: 10.1007/s00191-018-0557-5']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects = soup.find_all(\"div\", class_=\"project-content listing-pub-info\")\n",
    "projects = [x.text.strip() for x in projects]\n",
    "projects[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Years</th>\n",
       "      <th>Hosts</th>\n",
       "      <th>Winners</th>\n",
       "      <th>Score</th>\n",
       "      <th>Runner's-up</th>\n",
       "      <th>Third place</th>\n",
       "      <th>Score.1</th>\n",
       "      <th>Fourth place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1930 Details</td>\n",
       "      <td>Uruguay</td>\n",
       "      <td>Uruguay</td>\n",
       "      <td>4 - 2</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>United States</td>\n",
       "      <td>[note 1]</td>\n",
       "      <td>Yugoslavia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1934 Details</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Italy</td>\n",
       "      <td>2 - 1</td>\n",
       "      <td>Czechoslovakia</td>\n",
       "      <td>Germany</td>\n",
       "      <td>3 - 2</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1938 Details</td>\n",
       "      <td>France</td>\n",
       "      <td>Italy</td>\n",
       "      <td>4 - 2</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>4 - 2</td>\n",
       "      <td>Sweden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1950 Details</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Uruguay</td>\n",
       "      <td>2 - 1</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>[note 2]</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954 Details</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>West Germany</td>\n",
       "      <td>3 - 2</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>Austria</td>\n",
       "      <td>3 - 1</td>\n",
       "      <td>Uruguay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Years        Hosts       Winners  Score     Runner's-up  \\\n",
       "0  1930 Details      Uruguay       Uruguay  4 - 2       Argentina   \n",
       "1  1934 Details        Italy         Italy  2 - 1  Czechoslovakia   \n",
       "2  1938 Details       France         Italy  4 - 2         Hungary   \n",
       "3  1950 Details       Brazil       Uruguay  2 - 1          Brazil   \n",
       "4  1954 Details  Switzerland  West Germany  3 - 2         Hungary   \n",
       "\n",
       "     Third place   Score.1 Fourth place  \n",
       "0  United States  [note 1]   Yugoslavia  \n",
       "1        Germany     3 - 2      Austria  \n",
       "2         Brazil     4 - 2       Sweden  \n",
       "3         Sweden  [note 2]        Spain  \n",
       "4        Austria     3 - 1      Uruguay  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list = pd.read_html(\n",
    "    \"https://simple.wikipedia.org/wiki/FIFA_World_Cup\", match=\"Sweden\"\n",
    ")\n",
    "# Retrieve first and only entry from list of dataframes\n",
    "df = df_list[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdftotext\n",
      "  Using cached pdftotext-2.2.2.tar.gz (113 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pdftotext\n",
      "  Building wheel for pdftotext (setup.py): started\n",
      "  Building wheel for pdftotext (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pdftotext\n",
      "Failed to build pdftotext\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [13 lines of output]\n",
      "      WARNING: pkg-config not found--guessing at poppler version.\n",
      "               If the build fails, install pkg-config and try again.\n",
      "      WARNING: pkg-config not found--guessing at poppler version.\n",
      "               If the build fails, install pkg-config and try again.\n",
      "      WARNING: pkg-config not found--guessing at poppler version.\n",
      "               If the build fails, install pkg-config and try again.\n",
      "      C:\\Users\\admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\setuptools\\_distutils\\dist.py:261: UserWarning: Unknown distribution option: 'test_suite'\n",
      "        warnings.warn(msg)\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'pdftotext' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pdftotext\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pdftotext)\n"
     ]
    }
   ],
   "source": [
    "%pip install pdftotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading imdb top 250 movie's data\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = soup.select('td.titleColumn')\n",
    "crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
    "ratings = [b.attrs.get('data-value')\n",
    "\t\tfor b in soup.select('td.posterColumn span[name=ir]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a empty list for storing\n",
    "# movie information\n",
    "list = []\n",
    "\n",
    "# Iterating over movies to extract\n",
    "# each movie's details\n",
    "for index in range(0, len(movies)):\n",
    "\t\n",
    "\t# Separating movie into: 'place',\n",
    "\t# 'title', 'year'\n",
    "\tmovie_string = movies[index].get_text()\n",
    "\tmovie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "\tmovie_title = movie[len(str(index))+1:-7]\n",
    "\tyear = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "\tplace = movie[:len(str(index))-(len(movie))]\n",
    "\tdata = {\"place\": place,\n",
    "\t\t\t\"movie_title\": movie_title,\n",
    "\t\t\t\"rating\": ratings[index],\n",
    "\t\t\t\"year\": year,\n",
    "\t\t\t\"star_cast\": crew[index],\n",
    "\t\t\t}\n",
    "\tlist.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in list:\n",
    "\tprint(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n",
    "\t\t') -', 'Starring:', movie['star_cast'], movie['rating'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the list as dataframe\n",
    "#then converting into .csv file\n",
    "df = pd.DataFrame(list)\n",
    "df.to_csv('imdb_top_250_movies.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Downloading imdb top 250 movie's data\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "movies = soup.select('td.titleColumn')\n",
    "crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
    "ratings = [b.attrs.get('data-value')\n",
    "\t\tfor b in soup.select('td.posterColumn span[name=ir]')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create a empty list for storing\n",
    "# movie information\n",
    "list = []\n",
    "\n",
    "# Iterating over movies to extract\n",
    "# each movie's details\n",
    "for index in range(0, len(movies)):\n",
    "\t\n",
    "\t# Separating movie into: 'place',\n",
    "\t# 'title', 'year'\n",
    "\tmovie_string = movies[index].get_text()\n",
    "\tmovie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "\tmovie_title = movie[len(str(index))+1:-7]\n",
    "\tyear = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "\tplace = movie[:len(str(index))-(len(movie))]\n",
    "\tdata = {\"place\": place,\n",
    "\t\t\t\"movie_title\": movie_title,\n",
    "\t\t\t\"rating\": ratings[index],\n",
    "\t\t\t\"year\": year,\n",
    "\t\t\t\"star_cast\": crew[index],\n",
    "\t\t\t}\n",
    "\tlist.append(data)\n",
    "\n",
    "# printing movie details with its rating.\n",
    "for movie in list:\n",
    "\tprint(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n",
    "\t\t') -', 'Starring:', movie['star_cast'], movie['rating'])\n",
    "\n",
    "\n",
    "##.......##\n",
    "df = pd.DataFrame(list)\n",
    "df.to_csv('imdb_top_250_movies.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网页内容加载成功！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    " \n",
    "# 定义请求的 URL 和 headers\n",
    "url = \"https://movie.douban.com/top250\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    " \n",
    "# 发送 GET 请求\n",
    "response = requests.get(url, headers=headers)\n",
    "response.encoding = 'utf-8'  # 设置编码方式\n",
    "html_content = response.text  # 获取网页的 HTML 内容\n",
    "print(\"网页内容加载成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据提取成功！\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    " \n",
    "# 使用 Beautiful Soup 解析 HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    " \n",
    "# 提取电影名称、描述、评分和评价人数\n",
    "movies = []\n",
    "for item in soup.find_all('div', class_='item'):\n",
    "    title = item.find('span', class_='title').get_text()  # 电影名称\n",
    "    description = item.find('span', class_='inq')  # 电影描述\n",
    "    rating = item.find('span', class_='rating_num').get_text()  # 评分\n",
    "    votes = item.find('div', class_='star').find_all('span')[3].get_text()  # 评价人数\n",
    "    \n",
    "    # 如果没有描述，将其置为空字符串\n",
    "    if description:\n",
    "        description = description.get_text()\n",
    "    else:\n",
    "        description = ''\n",
    "    \n",
    "    movie = {\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "        \"rating\": rating,\n",
    "        \"votes\": votes.replace('人评价', '').strip()\n",
    "    }\n",
    "    movies.append(movie)\n",
    " \n",
    "print(\"数据提取成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功保存到 douban_top250.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    " \n",
    "# 将数据保存到 CSV 文件\n",
    "with open('douban_top250.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['title', 'description', 'rating', 'votes']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    " \n",
    "    writer.writeheader()  # 写入表头\n",
    "    for movie in movies:\n",
    "        writer.writerow(movie)  # 写入每一行数据\n",
    " \n",
    "print(\"数据已成功保存到 douban_top250.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功保存到 douban_top250.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    " \n",
    "# 定义请求的 URL 和 headers\n",
    "url = \"https://movie.douban.com/top250\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    " \n",
    "# 发送 GET 请求\n",
    "response = requests.get(url, headers=headers)\n",
    "response.encoding = 'utf-8'  # 设置编码方式\n",
    "html_content = response.text  # 获取网页的 HTML 内容\n",
    " \n",
    "# 使用 Beautiful Soup 解析 HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    " \n",
    "# 提取电影名称、描述、评分和评价人数\n",
    "movies = []\n",
    "for item in soup.find_all('div', class_='item'):\n",
    "    title = item.find('span', class_='title').get_text()  # 电影名称\n",
    "    description = item.find('span', class_='inq')  # 电影描述\n",
    "    rating = item.find('span', class_='rating_num').get_text()  # 评分\n",
    "    votes = item.find('div', class_='star').find_all('span')[3].get_text()  # 评价人数\n",
    "    \n",
    "    # 如果没有描述，将其置为空字符串\n",
    "    if description:\n",
    "        description = description.get_text()\n",
    "    else:\n",
    "        description = ''\n",
    "    \n",
    "    movie = {\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "        \"rating\": rating,\n",
    "        \"votes\": votes.replace('人评价', '').strip()\n",
    "    }\n",
    "    movies.append(movie)\n",
    " \n",
    "# 将数据保存到 CSV 文件\n",
    "with open('douban_top250.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['title', 'description', 'rating', 'votes']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    " \n",
    "    writer.writeheader()  # 写入表头\n",
    "    for movie in movies:\n",
    "        writer.writerow(movie)  # 写入每一行数据\n",
    " \n",
    "print(\"数据已成功保存到 douban_top250.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
